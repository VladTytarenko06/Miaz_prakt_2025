{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd6b7055",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'TrainingData.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     29\u001b[39m TOP_N = \u001b[32m200\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# 2) Завантаження та підготовка даних\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Читаємо CSV\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m kdata = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mINPUT_CSV\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Обираємо потрібні колонки (як у R)\u001b[39;00m\n\u001b[32m     38\u001b[39m base_cols = [\u001b[33m\"\u001b[39m\u001b[33mID\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmonth\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33myear\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mthree_sentences\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'TrainingData.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pathlib import Path\n",
    "\n",
    "# -----------------------\n",
    "# 1) Налаштування\n",
    "# -----------------------\n",
    "INPUT_CSV = \"TrainingData.csv\"\n",
    "OUTPUT_CSV = \"Output.csv\"\n",
    "\n",
    "# ВАШ список змінних-мітенів (як у R)\n",
    "list_variables = [\n",
    "    \"THREAT_up\",\n",
    "    \"THREAT_down\",\n",
    "    \"citizen_impact\",\n",
    "    \"PF_score\",\n",
    "    \"PF_US\",\n",
    "    \"PF_neg\",\n",
    "]\n",
    "\n",
    "# Скільки перших рядків вважаємо train (як у R)\n",
    "training_row = 300\n",
    "\n",
    "# Кількість слів у словнику (як words.to.keep у R)\n",
    "TOP_N = 200\n",
    "\n",
    "# -----------------------\n",
    "# 2) Завантаження та підготовка даних\n",
    "# -----------------------\n",
    "# Читаємо CSV\n",
    "kdata = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "# Обираємо потрібні колонки (як у R)\n",
    "base_cols = [\"ID\", \"month\", \"year\", \"three_sentences\"]\n",
    "missing = [c for c in base_cols + list_variables if c not in kdata.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"У файлі відсутні колонки: {missing}\")\n",
    "\n",
    "kdata = kdata[base_cols + list_variables].copy()\n",
    "\n",
    "# Чистка тексту (нижній регістр; пунктуацію далі «з'їсть» токенізатор)\n",
    "kdata[\"cleaned.body\"] = (\n",
    "    kdata[\"three_sentences\"]\n",
    "    .fillna(\"\")\n",
    "    .astype(str)\n",
    "    .str.lower()\n",
    ")\n",
    "\n",
    "# Обчислюємо фактичний розмір train/test з урахуванням довжини датасету\n",
    "n_docs = len(kdata)\n",
    "if n_docs <= training_row:\n",
    "    raise ValueError(\n",
    "        f\"У датасеті лише {n_docs} рядків — повинно бути > {training_row} для наявності тестової частини.\"\n",
    "    )\n",
    "\n",
    "train_idx = slice(0, training_row)\n",
    "test_idx = slice(training_row, n_docs)\n",
    "\n",
    "docs = kdata[\"cleaned.body\"].tolist()\n",
    "\n",
    "# -----------------------\n",
    "# 3) Словник за документною частотою (аналог docfreq + top-200)\n",
    "# -----------------------\n",
    "# Спочатку лічильник присутності терміну в документі (binary=True)\n",
    "count_vec = CountVectorizer(stop_words=\"english\", binary=True)\n",
    "X_bin = count_vec.fit_transform(docs)\n",
    "vocab = np.array(count_vec.get_feature_names_out())\n",
    "\n",
    "# Документна частота = сума по стовпцю (оскільки binary=True)\n",
    "doc_freq = np.asarray(X_bin.sum(axis=0)).ravel()\n",
    "\n",
    "# Топ-N слів за спаданням doc_freq\n",
    "order = np.argsort(-doc_freq)\n",
    "top_terms = vocab[order[: min(TOP_N, len(vocab))]]\n",
    "top_terms_set = set(top_terms)\n",
    "\n",
    "# -----------------------\n",
    "# 4) TF-IDF за фіксованим словником top_terms (як dfm_tfidf у R на kdtm2)\n",
    "# -----------------------\n",
    "tfidf_vec = TfidfVectorizer(stop_words=\"english\", vocabulary=sorted(top_terms_set))\n",
    "X_tfidf = tfidf_vec.fit_transform(docs)  # fit на словнику, не на текстах (OK, словник фіксований)\n",
    "\n",
    "# Матриці для train/test\n",
    "X_train = X_tfidf[train_idx]\n",
    "X_test = X_tfidf[test_idx]\n",
    "\n",
    "# -----------------------\n",
    "# 5) Навчання SVM і прогноз імовірностей для кожної цілі\n",
    "# -----------------------\n",
    "# Ініціалізуємо колонки для прогнозів як NaN\n",
    "for target in list_variables:\n",
    "    col_name = f\"predicted.values_{target}\"\n",
    "    kdata[col_name] = np.nan\n",
    "\n",
    "# Допоміжна функція: привести y до бінарних міток {0,1} як у R\n",
    "def prepare_labels(y_raw):\n",
    "    \"\"\"\n",
    "    - Порожні/NaN -> 0\n",
    "    - Якщо значення виглядають як числа -> 0/1\n",
    "    - Інакше робимо лейбл-енкодинг і попереджаємо, якщо більше 2 класів\n",
    "    Повертає: y (np.array), name_of_positive_class (для вибору proba[:,1])\n",
    "    \"\"\"\n",
    "    y = pd.Series(y_raw).copy()\n",
    "    y = y.fillna(0)\n",
    "\n",
    "    # якщо значення рядкові, спробуємо привести \"0\"/\"1\" до int\n",
    "    def to_num(v):\n",
    "        try:\n",
    "            # прибираємо пробіли\n",
    "            s = str(v).strip()\n",
    "            # інколи можуть бути \"0.0\"/\"1.0\"\n",
    "            return int(float(s))\n",
    "        except:\n",
    "            return v\n",
    "\n",
    "    y = y.map(to_num)\n",
    "\n",
    "    # Якщо вже {0,1}\n",
    "    unique_vals = sorted(pd.Series(y).dropna().unique().tolist())\n",
    "    if set(unique_vals).issubset({0,1}):\n",
    "        return np.array(y, dtype=int), 1  # позитивний клас = 1\n",
    "\n",
    "    # Інакше робимо лейбл-енкодинг\n",
    "    le = LabelEncoder()\n",
    "    y_enc = le.fit_transform(y.astype(str))\n",
    "    classes = list(le.classes_)\n",
    "    if len(classes) != 2:\n",
    "        print(\n",
    "            f\"[УВАГА] Ціль має {len(classes)} клас(и): {classes}. \"\n",
    "            f\"SVM з proba коректно працюватиме, але інтерпретація 'позитивного' класу умовна.\"\n",
    "        )\n",
    "    # позитивним вважаємо клас з індексом 1 (другий у лексикографічному порядку)\n",
    "    positive_label_name = classes[1] if len(classes) >= 2 else classes[0]\n",
    "    return y_enc, positive_label_name\n",
    "\n",
    "# Тренуємо моделі\n",
    "for target in list_variables:\n",
    "    # y для train\n",
    "    y_raw = kdata.loc[train_idx, target]\n",
    "    y_train, positive_ref = prepare_labels(y_raw)\n",
    "\n",
    "    # Модель SVM (аналог ksvm(..., kernel=\"rbfdot\", C=50, prob.model=TRUE))\n",
    "    clf = SVC(kernel=\"rbf\", C=50, probability=True, random_state=42)\n",
    "\n",
    "    # За аналогією з cross=10 у ksvm: обчислимо 10-fold CV-accuracy (опційно)\n",
    "    try:\n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "        cv_scores = cross_val_score(clf, X_train, y_train, cv=skf, scoring=\"accuracy\")\n",
    "        print(f\"[{target}] CV (10-fold) accuracy: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[{target}] Неможливо порахувати CV: {e}\")\n",
    "\n",
    "    # Навчання на всьому train і прогноз для test\n",
    "    clf.fit(X_train, y_train)\n",
    "    proba = clf.predict_proba(X_test)  # shape: (n_test, n_classes)\n",
    "\n",
    "    # Вибираємо \"другу\" колонку як у R: predict(..., type=\"probabilities\")[,2]\n",
    "    # У sklearn другий стовпчик відповідає класу clf.classes_[1]\n",
    "    # Якщо ми кодували у {0,1} — це буде proba класу 1 (тобто \"позитивний\")\n",
    "    positive_class_index = 1 if proba.shape[1] > 1 else 0\n",
    "    pos_proba = proba[:, positive_class_index]\n",
    "\n",
    "    # Записуємо у колонку predicted.values_<target> тільки для тестової частини\n",
    "    kdata.loc[test_idx, f\"predicted.values_{target}\"] = pos_proba\n",
    "\n",
    "# -----------------------\n",
    "# 6) Прибирання технічної колонки і збереження\n",
    "# -----------------------\n",
    "kdata = kdata.drop(columns=[\"cleaned.body\"])\n",
    "kdata.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"Готово! Результат збережено у {Path(OUTPUT_CSV).resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3631a09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.2-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\5103_6\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (2.3.1)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.16.2-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
      "     ------------ ------------------------- 20.5/60.8 kB 330.3 kB/s eta 0:00:01\n",
      "     ------------------------- ------------ 41.0/60.8 kB 495.5 kB/s eta 0:00:01\n",
      "     -------------------------------------- 60.8/60.8 kB 543.8 kB/s eta 0:00:00\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.7.2-cp311-cp311-win_amd64.whl (8.9 MB)\n",
      "   ---------------------------------------- 0.0/8.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.1/8.9 MB 4.3 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.3/8.9 MB 3.8 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.7/8.9 MB 5.5 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.3/8.9 MB 7.4 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.9/8.9 MB 8.1 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 2.5/8.9 MB 8.8 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 2.5/8.9 MB 8.8 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 2.8/8.9 MB 7.1 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 3.5/8.9 MB 8.0 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 4.6/8.9 MB 9.9 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 5.2/8.9 MB 10.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 5.8/8.9 MB 10.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.4/8.9 MB 10.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.9/8.9 MB 10.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.4/8.9 MB 10.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.0/8.9 MB 10.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 8.6/8.9 MB 10.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.9/8.9 MB 10.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.9/8.9 MB 10.5 MB/s eta 0:00:00\n",
      "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "   ---------------------------------------- 0.0/308.4 kB ? eta -:--:--\n",
      "   --------------------------------------  307.2/308.4 kB 18.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 308.4/308.4 kB 9.6 MB/s eta 0:00:00\n",
      "Downloading scipy-1.16.2-cp311-cp311-win_amd64.whl (38.7 MB)\n",
      "   ---------------------------------------- 0.0/38.7 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.6/38.7 MB 12.2 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 1.1/38.7 MB 11.5 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 1.5/38.7 MB 12.0 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 1.9/38.7 MB 11.1 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 2.4/38.7 MB 11.9 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 2.9/38.7 MB 11.7 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 3.5/38.7 MB 11.8 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 4.1/38.7 MB 11.8 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 4.7/38.7 MB 11.9 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 5.2/38.7 MB 11.7 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 5.7/38.7 MB 11.7 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 6.2/38.7 MB 11.7 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 6.8/38.7 MB 11.8 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 7.4/38.7 MB 11.8 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 7.8/38.7 MB 11.9 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 8.4/38.7 MB 11.9 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 9.0/38.7 MB 11.7 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 9.5/38.7 MB 11.9 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 10.1/38.7 MB 11.8 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 10.5/38.7 MB 11.7 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 11.0/38.7 MB 11.7 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 11.5/38.7 MB 11.9 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 12.0/38.7 MB 11.7 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 12.5/38.7 MB 11.9 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 13.1/38.7 MB 11.7 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 13.6/38.7 MB 11.7 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 14.1/38.7 MB 11.9 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 14.7/38.7 MB 11.7 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 15.2/38.7 MB 11.7 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 15.8/38.7 MB 11.9 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 16.2/38.7 MB 11.7 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 16.8/38.7 MB 11.7 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 17.2/38.7 MB 11.7 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 17.7/38.7 MB 11.7 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 18.2/38.7 MB 11.7 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 18.8/38.7 MB 11.9 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 19.3/38.7 MB 11.5 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 19.9/38.7 MB 11.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 20.4/38.7 MB 11.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 20.9/38.7 MB 11.7 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 21.5/38.7 MB 11.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 22.1/38.7 MB 11.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 22.7/38.7 MB 11.7 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 23.3/38.7 MB 11.7 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 23.9/38.7 MB 11.7 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 24.5/38.7 MB 11.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 25.0/38.7 MB 11.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 25.2/38.7 MB 11.7 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 25.2/38.7 MB 11.7 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 26.6/38.7 MB 11.7 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 27.2/38.7 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 27.8/38.7 MB 11.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 28.3/38.7 MB 11.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 28.7/38.7 MB 11.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 29.1/38.7 MB 11.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 29.7/38.7 MB 11.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 30.2/38.7 MB 11.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 30.9/38.7 MB 11.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 31.4/38.7 MB 11.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 31.8/38.7 MB 11.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 32.4/38.7 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 32.9/38.7 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 33.4/38.7 MB 11.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 34.0/38.7 MB 11.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 34.6/38.7 MB 11.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 35.0/38.7 MB 11.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 35.5/38.7 MB 12.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 36.0/38.7 MB 12.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 36.6/38.7 MB 11.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 37.1/38.7 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  37.7/38.7 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.3/38.7 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.7/38.7 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.7/38.7 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.7/38.7 MB 10.7 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.5.2 scikit-learn-1.7.2 scipy-1.16.2 threadpoolctl-3.6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\5103_6\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "589d776b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Файл TrainingData.csv створено в папці data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 1. Створюємо папку data\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# 2. Вміст CSV\n",
    "csv_content = \"\"\"ID,month,year,three_sentences,THREAT_up,THREAT_down,citizen_impact,PF_score,PF_US,PF_neg\n",
    "1,Jan,2025,\"Threat is increasing in the north sector.\",1,0,0,0,0,0\n",
    "2,Jan,2025,\"No threat detected in this area.\",0,0,0,0,0,0\n",
    "3,Jan,2025,\"Citizens reported some impact on infrastructure.\",0,0,1,0,0,0\n",
    "4,Jan,2025,\"Power failure reported in southern district.\",0,0,0,0,0,0\n",
    "5,Jan,2025,\"No negative sentiment detected in reports.\",0,0,0,0,0,1\n",
    "6,Jan,2025,\"US related issues are increasing according to data.\",1,0,0,0,1,0\n",
    "7,Jan,2025,\"Threat level down in eastern front.\",0,1,0,0,0,0\n",
    "8,Jan,2025,\"Citizen impact is minimal at this stage.\",0,0,1,0,0,0\n",
    "9,Jan,2025,\"PF score shows moderate risk.\",1,0,0,1,0,0\n",
    "10,Jan,2025,\"Everything seems calm, no events.\",0,0,0,0,0,0\n",
    "\"\"\"\n",
    "\n",
    "# 3. Створюємо файл у папці data\n",
    "with open(\"data/TrainingData.csv\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(csv_content)\n",
    "\n",
    "print(\"✅ Файл TrainingData.csv створено в папці data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  training_row зменшено до 5, бо рядків у файлі лише 10.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [5, 6]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 90\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(y)) > \u001b[32m1\u001b[39m:\n\u001b[32m     89\u001b[39m     skf = StratifiedKFold(n_splits=\u001b[38;5;28mmin\u001b[39m(\u001b[32m5\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(y))*\u001b[32m2\u001b[39m), shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m     scores = \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maccuracy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] CV accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscores.mean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ± \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscores.std()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     93\u001b[39m clf.fit(X_train, y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py:677\u001b[39m, in \u001b[36mcross_val_score\u001b[39m\u001b[34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[39m\n\u001b[32m    674\u001b[39m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[32m    675\u001b[39m scorer = check_scoring(estimator, scoring=scoring)\n\u001b[32m--> \u001b[39m\u001b[32m677\u001b[39m cv_results = \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    680\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mscore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[33m\"\u001b[39m\u001b[33mtest_score\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py:344\u001b[39m, in \u001b[36mcross_validate\u001b[39m\u001b[34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[39m\n\u001b[32m    141\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Evaluate metric(s) by cross-validation and also record fit/score times.\u001b[39;00m\n\u001b[32m    142\u001b[39m \n\u001b[32m    143\u001b[39m \u001b[33;03mRead more in the :ref:`User Guide <multimetric_cross_validation>`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    340\u001b[39m \u001b[33;03m[0.28009951 0.3908844  0.22784907]\u001b[39;00m\n\u001b[32m    341\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    342\u001b[39m _check_groups_routing_disabled(groups)\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m X, y = \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    345\u001b[39m params = {} \u001b[38;5;28;01mif\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m params\n\u001b[32m    346\u001b[39m cv = check_cv(cv, y, classifier=is_classifier(estimator))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:530\u001b[39m, in \u001b[36mindexable\u001b[39m\u001b[34m(*iterables)\u001b[39m\n\u001b[32m    500\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[32m    501\u001b[39m \n\u001b[32m    502\u001b[39m \u001b[33;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    526\u001b[39m \u001b[33;03m[[1, 2, 3], array([2, 3, 4]), None, <...Sparse...dtype 'int64'...shape (3, 1)>]\u001b[39;00m\n\u001b[32m    527\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    529\u001b[39m result = [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[32m--> \u001b[39m\u001b[32m530\u001b[39m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:473\u001b[39m, in \u001b[36mcheck_consistent_length\u001b[39m\u001b[34m(*arrays)\u001b[39m\n\u001b[32m    471\u001b[39m lengths = [_num_samples(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m arrays \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(lengths)) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    474\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    475\u001b[39m         % [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[32m    476\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Found input variables with inconsistent numbers of samples: [5, 6]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pathlib import Path\n",
    "\n",
    "# -----------------------\n",
    "# 1) Налаштування\n",
    "# -----------------------\n",
    "INPUT_CSV = \"data/TrainingData.csv\"\n",
    "OUTPUT_CSV = \"Output.csv\"\n",
    "\n",
    "# Список цільових змінних\n",
    "list_variables = [\"THREAT_up\", \"THREAT_down\", \"citizen_impact\", \"PF_score\", \"PF_US\", \"PF_neg\"]\n",
    "\n",
    "# Скільки перших рядків беремо на тренування\n",
    "training_row = 300\n",
    "TOP_N = 200\n",
    "\n",
    "# -----------------------\n",
    "# 0) Якщо файлу немає – створюємо тестовий\n",
    "# -----------------------\n",
    "if not os.path.exists(INPUT_CSV):\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    csv_content = \"\"\"ID,month,year,three_sentences,THREAT_up,THREAT_down,citizen_impact,PF_score,PF_US,PF_neg\n",
    "1,Jan,2025,\"Threat is increasing in the north sector.\",1,0,0,0,0,0\n",
    "2,Jan,2025,\"No threat detected in this area.\",0,0,0,0,0,0\n",
    "3,Jan,2025,\"Citizens reported some impact on infrastructure.\",0,0,1,0,0,0\n",
    "4,Jan,2025,\"Power failure reported in southern district.\",0,0,0,0,0,0\n",
    "5,Jan,2025,\"No negative sentiment detected in reports.\",0,0,0,0,0,1\n",
    "6,Jan,2025,\"US related issues are increasing according to data.\",1,0,0,0,1,0\n",
    "7,Jan,2025,\"Threat level down in eastern front.\",0,1,0,0,0,0\n",
    "8,Jan,2025,\"Citizen impact is minimal at this stage.\",0,0,1,0,0,0\n",
    "9,Jan,2025,\"PF score shows moderate risk.\",1,0,0,1,0,0\n",
    "10,Jan,2025,\"Everything seems calm, no events.\",0,0,0,0,0,0\n",
    "\"\"\"\n",
    "    with open(INPUT_CSV, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(csv_content)\n",
    "    print(f\"⚠️  Файл {INPUT_CSV} не знайдено — створено тестовий приклад із 10 рядками.\")\n",
    "\n",
    "# -----------------------\n",
    "# 2) Завантаження даних\n",
    "# -----------------------\n",
    "kdata = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "# Автоматично підлаштовуємо training_row якщо рядків менше 300\n",
    "if len(kdata) < training_row:\n",
    "    training_row = len(kdata) // 2\n",
    "    print(f\"⚠️  training_row зменшено до {training_row}, бо рядків у файлі лише {len(kdata)}.\")\n",
    "\n",
    "base_cols = [\"ID\", \"month\", \"year\", \"three_sentences\"]\n",
    "kdata = kdata[base_cols + list_variables].copy()\n",
    "\n",
    "# Чистимо текст\n",
    "kdata[\"cleaned.body\"] = kdata[\"three_sentences\"].fillna(\"\").astype(str).str.lower()\n",
    "\n",
    "docs = kdata[\"cleaned.body\"].tolist()\n",
    "n_docs = len(docs)\n",
    "train_idx = slice(0, training_row)\n",
    "test_idx = slice(training_row, n_docs)\n",
    "\n",
    "# -----------------------\n",
    "# 3) Топ-200 термінів за document frequency\n",
    "# -----------------------\n",
    "count_vec = CountVectorizer(stop_words=\"english\", binary=True)\n",
    "X_bin = count_vec.fit_transform(docs)\n",
    "vocab = np.array(count_vec.get_feature_names_out())\n",
    "doc_freq = np.asarray(X_bin.sum(axis=0)).ravel()\n",
    "\n",
    "order = np.argsort(-doc_freq)\n",
    "top_terms = vocab[order[:min(TOP_N, len(vocab))]]\n",
    "tfidf_vec = TfidfVectorizer(stop_words=\"english\", vocabulary=sorted(set(top_terms)))\n",
    "X_tfidf = tfidf_vec.fit_transform(docs)\n",
    "\n",
    "X_train = X_tfidf[train_idx]\n",
    "X_test = X_tfidf[test_idx]\n",
    "\n",
    "# -----------------------\n",
    "# 4) Навчання моделей\n",
    "# -----------------------\n",
    "for target in list_variables:\n",
    "    y = kdata.loc[train_idx, target].fillna(0).astype(int)\n",
    "\n",
    "    clf = SVC(kernel=\"rbf\", C=50, probability=True, random_state=42)\n",
    "    if len(set(y)) > 1:\n",
    "        skf = StratifiedKFold(n_splits=min(5, len(set(y))*2), shuffle=True, random_state=42)\n",
    "        scores = cross_val_score(clf, X_train, y, cv=skf, scoring=\"accuracy\")\n",
    "        print(f\"[{target}] CV accuracy: {scores.mean():.3f} ± {scores.std():.3f}\")\n",
    "\n",
    "    clf.fit(X_train, y)\n",
    "    if X_test.shape[0] > 0:\n",
    "        probs = clf.predict_proba(X_test)[:, 1]  # ймовірність класу 1\n",
    "        kdata.loc[test_idx, f\"predicted.values_{target}\"] = probs\n",
    "    else:\n",
    "        kdata[f\"predicted.values_{target}\"] = np.nan\n",
    "\n",
    "kdata = kdata.drop(columns=[\"cleaned.body\"])\n",
    "kdata.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"✅ Готово! Результат збережено у {Path(OUTPUT_CSV).resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f8a94cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Docs: 10 | Train: 5 | Test: 5 | Vocab: 33\n",
      "\n",
      "🧠 Модель для цілі: THREAT_up\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Несумісні розміри: X_train=5 vs y_train=6. Перевірте формування train_idx/test_idx.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 134\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;66;03m# Перевірка узгодженості\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m X_train.shape[\u001b[32m0\u001b[39m] != \u001b[38;5;28mlen\u001b[39m(y_train):\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    135\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mНесумісні розміри: X_train=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train.shape[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m vs y_train=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(y_train)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    136\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mПеревірте формування train_idx/test_idx.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    137\u001b[39m     )\n\u001b[32m    139\u001b[39m clf = SVC(kernel=\u001b[33m\"\u001b[39m\u001b[33mrbf\u001b[39m\u001b[33m\"\u001b[39m, C=\u001b[32m50\u001b[39m, probability=\u001b[38;5;28;01mTrue\u001b[39;00m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# Безпечний CV (за наявності достатніх даних)\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Несумісні розміри: X_train=5 vs y_train=6. Перевірте формування train_idx/test_idx."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# -----------------------\n",
    "# 1) Налаштування\n",
    "# -----------------------\n",
    "INPUT_CSV = \"data/TrainingData.csv\"\n",
    "OUTPUT_CSV = \"Output.csv\"\n",
    "LIST_VARIABLES = [\"THREAT_up\", \"THREAT_down\", \"citizen_impact\", \"PF_score\", \"PF_US\", \"PF_neg\"]\n",
    "TOP_N = 200          # скільки термінів лишити за docfreq\n",
    "DEFAULT_TRAIN = 300  # як у R-скрипті\n",
    "\n",
    "# -----------------------\n",
    "# 0) Автогенерація тестових даних, якщо файлу немає\n",
    "# -----------------------\n",
    "def ensure_sample_csv(path: str):\n",
    "    if os.path.exists(path):\n",
    "        return\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    csv_content = \"\"\"ID,month,year,three_sentences,THREAT_up,THREAT_down,citizen_impact,PF_score,PF_US,PF_neg\n",
    "1,Jan,2025,\"Threat is increasing in the north sector.\",1,0,0,0,0,0\n",
    "2,Jan,2025,\"No threat detected in this area.\",0,0,0,0,0,0\n",
    "3,Jan,2025,\"Citizens reported some impact on infrastructure.\",0,0,1,0,0,0\n",
    "4,Jan,2025,\"Power failure reported in southern district.\",0,0,0,0,0,0\n",
    "5,Jan,2025,\"No negative sentiment detected in reports.\",0,0,0,0,0,1\n",
    "6,Jan,2025,\"US related issues are increasing according to data.\",1,0,0,0,1,0\n",
    "7,Jan,2025,\"Threat level down in eastern front.\",0,1,0,0,0,0\n",
    "8,Jan,2025,\"Citizen impact is minimal at this stage.\",0,0,1,0,0,0\n",
    "9,Jan,2025,\"PF score shows moderate risk.\",1,0,0,1,0,0\n",
    "10,Jan,2025,\"Everything seems calm, no events.\",0,0,0,0,0,0\n",
    "\"\"\"\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(csv_content)\n",
    "    print(f\"⚠️  {path} не знайдено — створено тестовий файл із 10 рядками.\")\n",
    "\n",
    "ensure_sample_csv(INPUT_CSV)\n",
    "\n",
    "# -----------------------\n",
    "# 2) Завантаження та базова підготовка\n",
    "# -----------------------\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "base_cols = [\"ID\", \"month\", \"year\", \"three_sentences\"]\n",
    "\n",
    "missing_cols = [c for c in base_cols + LIST_VARIABLES if c not in df.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"У файлі відсутні необхідні колонки: {missing_cols}\")\n",
    "\n",
    "df = df[base_cols + LIST_VARIABLES].copy()\n",
    "df[\"cleaned.body\"] = df[\"three_sentences\"].fillna(\"\").astype(str).str.lower()\n",
    "\n",
    "n_docs = len(df)\n",
    "if n_docs < 2:\n",
    "    raise ValueError(\"Потрібно щонайменше 2 рядки (один для train, один для test).\")\n",
    "\n",
    "# training_row: якщо мало рядків — беремо половину (мінімум 1)\n",
    "training_row = min(DEFAULT_TRAIN, max(1, n_docs // 2))\n",
    "train_idx = slice(0, training_row)\n",
    "test_idx = slice(training_row, n_docs)\n",
    "\n",
    "docs = df[\"cleaned.body\"].tolist()\n",
    "\n",
    "# -----------------------\n",
    "# 3) Словник за document frequency → TF-IDF\n",
    "# -----------------------\n",
    "count_vec = CountVectorizer(stop_words=\"english\", binary=True)\n",
    "X_bin = count_vec.fit_transform(docs)\n",
    "vocab = np.array(count_vec.get_feature_names_out())\n",
    "\n",
    "if vocab.size == 0:\n",
    "    raise ValueError(\"Після токенізації словник порожній. Перевірте тексти або вимкніть стоп-слова.\")\n",
    "\n",
    "doc_freq = np.asarray(X_bin.sum(axis=0)).ravel()\n",
    "order = np.argsort(-doc_freq)\n",
    "top_terms = vocab[order[: min(TOP_N, vocab.size)]]\n",
    "vocab_fixed = sorted(set(top_terms))\n",
    "\n",
    "tfidf_vec = TfidfVectorizer(stop_words=\"english\", vocabulary=vocab_fixed)\n",
    "X_tfidf = tfidf_vec.fit_transform(docs)\n",
    "\n",
    "X_train = X_tfidf[train_idx]\n",
    "X_test = X_tfidf[test_idx]\n",
    "\n",
    "print(f\"📊 Docs: {n_docs} | Train: {X_train.shape[0]} | Test: {X_test.shape[0]} | Vocab: {len(vocab_fixed)}\")\n",
    "\n",
    "# -----------------------\n",
    "# 4) Допоміжні функції\n",
    "# -----------------------\n",
    "def to_binary_labels(series: pd.Series) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Перетворює ціль у бінарні мітки {0,1}.\n",
    "    НЧ/порожні → 0. Будь-що не 0 → 1.\n",
    "    \"\"\"\n",
    "    y = pd.to_numeric(series, errors=\"coerce\").fillna(0)\n",
    "    y = (y != 0).astype(int)\n",
    "    return y.to_numpy()\n",
    "\n",
    "def safe_cv(clf, X, y) -> None:\n",
    "    \"\"\"\n",
    "    Обчислює k-fold CV тільки якщо вистачає даних у найменшому класі.\n",
    "    \"\"\"\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    class_counts = dict(zip(unique.tolist(), counts.tolist()))\n",
    "    if len(unique) < 2:\n",
    "        print(f\"• CV пропущено: лише один клас у train. Класи: {class_counts}\")\n",
    "        return\n",
    "    min_class = counts.min()\n",
    "    n_train = X.shape[0]\n",
    "    # Маємо мати щонайменше 2 зразки в мінімальному класі і ≥4 загалом\n",
    "    if min_class < 2 or n_train < 4:\n",
    "        print(f\"• CV пропущено: замало даних (min_class={min_class}, n_train={n_train}). Класи: {class_counts}\")\n",
    "        return\n",
    "    n_splits = min(5, int(min_class))\n",
    "    if n_splits < 2:\n",
    "        print(f\"• CV пропущено: n_splits={n_splits} < 2.\")\n",
    "        return\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(clf, X, y, cv=skf, scoring=\"accuracy\")\n",
    "    print(f\"• CV ({n_splits}-fold) accuracy: {scores.mean():.3f} ± {scores.std():.3f}\")\n",
    "\n",
    "# -----------------------\n",
    "# 5) Навчання моделей і прогнози\n",
    "# -----------------------\n",
    "for target in LIST_VARIABLES:\n",
    "    print(f\"\\n🧠 Модель для цілі: {target}\")\n",
    "    y_train = to_binary_labels(df.loc[train_idx, target])\n",
    "\n",
    "    # Перевірка узгодженості\n",
    "    if X_train.shape[0] != len(y_train):\n",
    "        raise ValueError(\n",
    "            f\"Несумісні розміри: X_train={X_train.shape[0]} vs y_train={len(y_train)}. \"\n",
    "            f\"Перевірте формування train_idx/test_idx.\"\n",
    "        )\n",
    "\n",
    "    clf = SVC(kernel=\"rbf\", C=50, probability=True, random_state=42)\n",
    "\n",
    "    # Безпечний CV (за наявності достатніх даних)\n",
    "    safe_cv(clf, X_train, y_train)\n",
    "\n",
    "    # Навчання на всьому train\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Прогнози для тесту (ймовірність класу 1)\n",
    "    col_name = f\"predicted.values_{target}\"\n",
    "    df[col_name] = np.nan\n",
    "    if X_test.shape[0] > 0:\n",
    "        proba = clf.predict_proba(X_test)\n",
    "        # індекс класу \"1\" (якщо чомусь його немає — беремо останню колонку)\n",
    "        classes = clf.classes_\n",
    "        pos_index = int(np.where(classes == 1)[0][0]) if 1 in classes else (proba.shape[1] - 1)\n",
    "        df.loc[test_idx, col_name] = proba[:, pos_index]\n",
    "    else:\n",
    "        print(\"• Тестових прикладів немає — прогноз пропущено.\")\n",
    "\n",
    "# -----------------------\n",
    "# 6) Збереження результату\n",
    "# -----------------------\n",
    "out_df = df.drop(columns=[\"cleaned.body\"])\n",
    "out_path = Path(OUTPUT_CSV).resolve()\n",
    "out_df.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "print(f\"\\n✅ Готово! Результат збережено у {out_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
